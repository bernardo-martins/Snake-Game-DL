Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. If in doubt you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

Atenção:
- Não edite este ficheiro em programas como Word e afins. Use exclusivamente um editor de texto simples. Em caso de dúvida, use o editor do Spyder.
- Não altere a estrutura deste ficheiro. Preencha as respostas apenas nos espaços respectivos (a seguir à tag R#:)
- Pode adicionar linhas no espaço para as respostas mas as respostas devem ser sucintas e directas.

Linking images and reports/Incluir imagens e relatórios
- You can link a .png or .html file in your answers by typing the name of the file in a separate line. The file must be in the same folder as this TP1.txt file. See the examples below:
- Pode ligar às respostas um ficheiro .png ou .html escrevendo o nome do ficheiro numa linha separada. O ficheiro tem de estar presente na mesma pasta em que está este ficheiro TP1.txt. Por exemplo:

exemplo.png
exemplo.html

PERGUNTAS/QUESTIONS:

Q1: Explain and justify the architecture of your agent, describing the network, input and output, and what experiments you did to choose this particular architecture.
Q1; Explique e justifique a arquitectura do seu agente, descrevendo a rede, os valores de entrada e saída e que experiências fez para escolher esta rede em particular.
R1:
A arquitetura escolhida para o agente é semelhante à arquitetura do primeiro trabalho, uma vez que este agente também terá de extrair features de uma imagem.

agente.png

Queremos mapear o estado atual em que se encontra o ambiente para a ação que maximiza as possíveis rewards futuras.
Como tal, o nosso agente terá de receber como input a imagem da board - (width, height, RGB) - e deverá fazer output dos valores previstos como reward máxima para as 3 ações possíveis (esquerda, frente, direita).
Logo, a rede recebe como input o state_shape (que neste caso é o board_shape) e como output terá o action_shape (que neste caso é 3 neurónios).
A primeira camada convolucional efetua duas convoluções com kernel size de (3,3) de modo a extrair features da imagem da board. Entre essas camadas fazemos MaxPooling para reduzir o tamanho da imagem e permitir detetar mais padrões com menos parâmetros. O objetivo é conseguir uma abstração da posição da cobra no referencial, para que a rede apenas se tenha de preocupar com a posição da cobra relativa à maçã. De seguida, a camada dense irá efetuar a classificação e no final são devolvidas as probabilidades de cada uma das ações ser a melhor. 
A função de ativação utilizada nas camadas escondidas foi a reLU, de forma a evitar o problema dos vanishing gradients. Na camada final utilizamos a ativação linear pois estamos perante um problema de regressão, em que queremos saber qual é o Q value de cada ação.
A função de Loss utilizada foi a Mean Squared Error que compara a diferença entre os Q value obtidos e os Q value previstos, tentando minimizar essa diferença.
O otimizador escolhido foi o otimizador Adam pois, pelas experiências feitas, era o que apresentava melhores resultados para o modelo descrito.
Como podemos observar pelo log do tensor board, a função de loss converge para valores baixos:

log_tensorboard.png

Q2: Describe the scenario for which you tried to optimize your agent, justifying your choice and describing the experiments you did to select this scenario.
Q2: Descreva o cenário para o qual tentou optimizar o seu agente, justificando a sua escolha e descrevendo que experiências fez para seleccionar este cenário.
R2:
De início, reparámos que o agente não sobrevivia muito tempo e não era capaz de aprender a comer a maçã. 

steps_first_try.png
apples_first_try.png

Isto devia-se ao facto de estarmos a utilizar uma board grande, sem relva e apenas com uma maçã. A partir deste momento, percebemos que era necessário apresentar casos mais “óbvios” ao agente em momentos iniciais para que o processo de aprendizagem fosse melhor.
Assim, inicialmente, a board é criada com uma dimensão de 32x32, sendo que após a border de 10, a dimensão real é 14x14. Queremos treinar o agente a aumentar o score procurando comer a maçã, e obtendo dessa maneira rewards, mas verificámos que comer a maçã é um evento raro na dimensão normal da board. Assim sendo decidimos diminuir o tamanho do espaço de jogo em que a cobra se desloca, o que potencia que esta tenha mais facilidade em encontrar a maçã numa fase inicial em que não tem conhecimento suficiente para saber que a maçã garante mais rewards. Como queremos que numa fase avançada do treino o jogo possa ser jogado numa imagem de 32x32 com border de 1, utilizamos a border para diminuir o espaço de jogo, habituando o agente a olhar para uma imagem com dimensões fixas, sem ter que mudar o input da rede. Outras técnicas que utilizamos foram aumentar o número de maçãs que estão na board (pois aumenta a probabilidade de comer uma maçã) e alterar os parâmetros que controlam a relva da board (para incentivar a cobra a procurar explorar novos locais da board, associando à relva não comida uma reward). 

Q3: Explain how you generate the experiences for training your agent. Do not forget to explain the details like the balance between exploitation and exploration, how experiences are selected, what information you store in each experience, how large is the pool of experiences and how frequently are new experiences generated. Justify your choices.
Q3: Explique como gerou as experiências para treinar seu agente. Não se esqueça de explicar os detalhes como o equilíbrio entre exploração aleatória e guiada (exploration e exploitation), como as experiências são selecionadas, que informação guarda em cada experiência, quão grande é o conjunto de experiências e com que frequência novas experiências são geradas. Justifique estas escolhas.
R3: 
As experiências geradas na pool inicial foram pensadas de forma a fornecer ao agente exemplos em que a cobra consegue comer a maçã e exemplos em que a cobra navega pelo mapa e eventualmente morre. Tal é importante para que a rede perceba que tipo de experiências conduzem a uma maximização do reward final e que experiências prejudicam a obtenção desse reward. Assim, de início, são geradas 100000 experiências. Inicialmente, experimentamos adicionar qualquer tipo de experiência vinda da heurística. No entanto, reparámos que o modelo fica com demasiados exemplos em que não acontece nada e poucos em que consegue comer a maçã ou em que morre, sendo que os exemplos em que não acontece nada não acrescentam muita informação, o que resultou no seguinte gráfico onde podemos ver que o número de passos dados foi muito inferior à versão final:

steps_all_heuristics.png

Assim, decidimos equilibrar a quantidade de experiências de cada tipo, com aproximadamente ⅓ de experiências em que a cobra morre, ⅓ em que a cobra come uma maçã e ⅓ em que não acontece nada, sendo que algumas experiências foram geradas a partir de ações random, de forma a introduzir mais erro e ocasiões em que a cobra morre.
A exploração aleatória e guiada é determinada pelo valor de epsilon que decai ao longo do tempo. De início, um valor elevado de epsílon (1) permite que o agente explore mais o ambiente, uma vez que ainda não tem uma política bem definida, o que permite aprender experiências novas. No entanto, à medida que o epsilon é reduzido por um fator de 0.001, as decisões passam a ser tomadas pelo agente, com o conhecimento adquirido previamente. Para que o agente não fique limitado ao seu conhecimento, definimos um epsilon mínimo de 0.01, para que 10% das ações sejam feitas de forma random.
Para cada passo que o agente dá, é guardada a experiência resultante desse passo. Uma vez que o lote de experiências está sempre cheio desde o ínicio, experiências mais antigas são substituídas pelas obtidas durante o treino. Cada uma destas experiências é constituída por tuplos [observation, action, reward, new_observation, done], e esta informação será utilizada no treino.

Q4: Explain how you trained your agent: the algorithm used, the discount factor (gamma), the schedule for generating new experiences and training the agent and other techniques. Justify your choices and explain what experiments you did and which alternatives you tried.
Q4: Explique como treinou o seu agente: o algoritmo utilizado, o factor de desconto (gama), a coordenação da geração de novas experiências e treinao do agente, e outras técnicas que tenha usado. Justifique suas escolhas e explique que alternativas testou e como validou experimentalmente estas escolhas.
R4: 
O algoritmo utilizado para treinar o agente foi o Q-Learning com um discount factor de 0.9. Optámos por não penalizar a cobra por apanhar a maçã tarde no jogo, uma vez que comer a maçã é uma ação rara e temos dificuldades em propagar a reward para posições mais afastadas da maçã. Contudo, as diferenças verificadas entre este discount facto e o de 0.6 não foram muitas  Decidimos utilizar o algoritmo Q learning pelas similaridades de performance que observámos relativamente ao SARSA nas mesmas condições de treino, pois o nosso agente procura sempre escolher a ação que maximiza o q value. Treinámos o agente sempre que este dava 256 passos ou morria. O objetivo do treino era que o modelo se adaptasse a escolher sempre a ação que obtém maiores rewards possíveis.
Tentámos ainda utilizar um decaimento do learning rate, procurando fazer com que o erro minímo fosse encontrado mais cedo, no entanto, após tentarmos vários níveis de decaimento para diferentes valores de learning rate, concluímos que a melhor estratégia passaria por manter um learning rate fixo.

Q5: Discuss this problem and your solution, describing the agent's performance (score, survival, etc.), which aspects of the game were most difficult for the agent to learn, how you tried to remedy these difficulties and what ideas you might have tried if you had more time.
Q5: Discuta este problema e a sua solução, descrevendo o desempenho do agente (pontuação, sobrevivência, etc), que aspectos do jogo foram mais difíceis de aprender para o agente, como tentou colmatar essas dificuldades e que ideias ficaram que poderia experimentar se tivesse mais tempo.
R5:
Este problema é complexo pois a reward dá um feedback positivo quando o agente come a maçã e um feedback negativo quando este morre, e ambas estas ocasiões estão em muito menor escala quando comparadas às ocasiões em que não existe qualquer reward. Assim sendo, é muito raro o agente obter rewards que encoragem o comportamento de perseguir a maçã, e também é muito difícil propagar essas rewards para estados em que a cobra está mais longe da maçã. É relativamente simples fazer com que a cobra não morra, pois conseguimos fazer com que ela morra através de ações random, o que irá dar um feedback negativo, e conseguimos incentivá-la a procurar novos caminhos utilizando relva, dando feedback positivo por ir para sítios onde ainda não esteve. Contudo, comer a maçã continua a ser um caso raro, mesmo aumentando o número de maçãs (que apresenta alguns resultados, mas não melhora o suficiente para que se apresente uma boa performance, no nosso caso).
Aquilo que acreditamos que poderiamos ter feito para melhorar a performance do nosso agente seria, em primeiro lugar, fazer com que as rewards por apanhar as maçãs fossem mais elevadas. Pensamos que com um acréscimo nas rewards de quando o agente apanha a maçã, a reward poderia propagar-se mais facilmente para os estados mais afastados. Em segundo lugar, poderíamos melhorar a distribuição de exemplos para o lote inicial que fizesse com que o agente tivesse experiências em todos os estados possíveis.
No final, o nosso modelo revelou-se competente o suficiente para manter a cobra viva durante bastante tempo, mas não para obter pontuações elevadas. Durante o treino, obtivemos os seguintes resultados:

apples_eaten_final.png
steps_final.png

E ao testar o modelo obtivemos os seguintes resultados (que mais uma vez confirmam que o modelo não é capaz de obter pontuação, mas mantém-se vivo por bastante tempo):

apples_eaten_test.png
steps_test.png

Enviamos também uma pequena demonstração dos resultados em vídeo (com alguns frames cortados, porque a animação não ficou perfeita):

snake_video.avi
